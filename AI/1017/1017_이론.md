## 딥러닝 및 이미지 파운데이션 모델
### AI 파운데이션 모델의 개념
#### 파운데이션 모델
- AI 모델
  - 함수 또는 프로그램
  - 입출력을 연결해주는 함수 + 데이터로 학습된 함수 + 학습 때 보지 못했던 데이터에 대해서도 작동해야 하는 의무
- 이상적인 AI 모델
  - AI모델이 이 세상에서 발생 가능한 모든 데이터와 각 데이터의 설명을 모두 기억하고 있는 모델
  - 내가 얻고 싶은 답과 유사한 답이 이미 DB에 저장되어 있을 확률이 높음 -> 검색 엔진과 유사
  - 그러나 데이터 확보, 저장, 탐색은 매우 비용이 크고 현실적이지 않음
- 현실적인 기계학습 모델
  - 학습 = AI모델에 데이터를 패턴화하여 압축
  - 이 과정에서 비슷함과 다름을 파악하게 되고, 패턴을 익히면서 새로운 데이터에 대한 일반화 능력이 생김
- 파운데이션 모델이란?
  - `대규모` 데이터를 폭넓게 `학습`한 후, 다양한 문제에 빠르게 `적응`할 수 있는 범용 대형 AI 모델
  - 미국 스탠포드 대학 사람 중심 AI 연구소에서 2021년 출간된 보고서에서 새로운 범주로 구분을 시작
- 파운데이션 모델
  - `기존 딥러닝 개발 패러다임`: 아기와 같이 언어, 시각, 청각, 촉각 등 기본적인 것들부터 배워 나가야 함
  - `파운데이션 모델 패러다임`: 거대 모델(커다란 뇌) + 대규모 데이터 학습 (많은 지식과 경험) 기반
    - `새로운 일을 처음 접해도 금방 배우고 잘할 수 있음`

#### 파운데이션 모델의 특징
- 특징 1 [`대규모`]: 트랜스포머 모델 + 대규모 언어 데이터 학습
  - 테스크에 상관없이 비슷한 패턴들이 등장하고 있음
  - 주로 비지도학습으로 훈련된 모델들"도" 많이 등장
    - 의미하는 바: `쉬운 데이터 수집` + `대규모 학습`
- 특징 2 [`적응성`]: 높은 파인튜닝 성능 (높은 태스크 적응 성능)
  - 믿고 쓸 수 있는 모델
- 특징 3 [`범용성`]: 다양한 작업, 한정되지 않는 출력 지원
  - 예시 - 물체 판별
    - 기존: `20`여개의 물체 종류 구분
    - 파운데이션 모델: `만`개 이상을 물체 종류 구분 (또는 자연어 기반의 한정되지 않은 대상에 대한 인식)

#### 파운데이션 모델에 의한 AI 모델 개발의 변화
- 과거에는 매번 모델을 새로 학습했지만, 이제는 잘 학습된 모델들을 얼마나 잘 활용하느냐가 핵심
- 파운데이션 모델 하나 확보하는데 투여되는 계산 리소스는 일부 대규모 인프라 이외에 불가
- 적응 활용
  - 활용 되는 기법들: `프롬프트`(엔지니어링, 튜닝), `전이학습`(Adaptation)학습, `파인튜닝`
    - `Zero-shot`: 처음 보는 문제를 추가 학습 없이 바로 적용 (모델 자체가 가진 배경 지식 활용)
    - `Few-shot`: 예제 몇 개만 보여주면 바로 적용 가능
    - `Fine-tuning`: 처음부터 배우지 않아도, 조금만 알려주면 금방 적응 (모델 자체를 업데이트, 모델 가중치가 변경 됨)

### 대표적 AI 파운데이션 모델 -CLIP-
#### AGI를 향해서
- Human's Intelligence (cognition) = perception ∪ higher cognitive processes
  - 2022년 11월 이전: 각 분야별로 지능의 매우 부분적 능력만을 개별적으로 모델링 시도
  - 2022년 11월 (ChatGPT) 이후: 대규모 언어모델(LLM)이 높은 사고/추론 성능을 보여주기 시작 (다양한 인지 능력 벤치마크에서 인간 수준 근접)

#### 시각언어모델 예시 - ChatGPT with GPT-4
- GPT-4 (2023)
  - 자연어 입력에 국한된 기존의 거대 언어 모델에서 더 나아가 이미지, 문서, 음성 등 `멀티모달(multi-modal)` 데이터를 처리할 수 있는 모델
  - GPT-4 API를 활용하여 다양한 도메인의 이미지 데이터와 결합한 모델이 개발됨 (예시 : 제조 AI)

#### CLIP (2021)
- CLIP: Contrastive Language-Image Pre-training, by OpenAI
  - AI가 언어와 시각을 통합해서 이해하는 방식을 보여준 패러다임 전환 제시
  - 파운데이션 모델로써의 특징
    - 입력: 학습하지 않은 새로운 `도메인`의 입력 데이터에 대해서도 좋은 성능을 발휘 (`제로샷 전이`)
    - 출력: 자연어를 이용해 한번도 본 적 없는 `카테고리`도 텍스트 설명만으로 출력 정의 가능 (`언어 인터페이스`)
- 대조 학습 기반(Contrastive Pre-training)의 언어-이미지 사전 학습
  - 인터넷 데이터를 통한 지도 학습(supervised learning)을 통해 자연어 기반 시각 개념 학습
  - 다양한 이미지-자연어 쌍으로 학습
    - 인터넷에서 수집된 `수억`개의 이미지-텍스트 쌍
    - 데이터 정제 과정을 거침 (중복 이미지, 해상도/품질 낮은 이미지, 짧은 텍스트 등)
  - 다양한 이미지-자연어 쌍으로 학습
    - 텍스트 인코더 : `Transformer`
    - 이미지 인코더 : `ViT-B`(또는 `ResNet50`)

#### CLIP 구조 - 텍스트 인코더 (Transformer 기반 Text Encoder)
- Remind - Transformer
  - `토큰`이라는 단위의 입력
  - 입력된 토큰 간의 관계성을 집중하는 `Attention` 메커니즘으로 구성
  - L 길이의 입력 토큰은 D-차원 특징벡터(임베딩)의 배열 형태로 입력 (L x D)
  - 트랜스포머 구조 = `인코더 (Encoder)` + `디코더 (Decoder)`
  - CLIP에서는 `Encoder only` 구조 사용
  - 자연어 데이터 : `Sub-word` 단위의 임베딩

#### CLIP 구조 - 이미지 인코더 (ViT: Vision Transformer, 2020)
- Remind - Vision Transformer
  - 이미지를 작은 `패치(16x16x3)`로 나눔
  - 각 패치를 1D로 `Flatten`
  - `Learnable position embedding` 사용
    - 이미지 내에서 각 패치의 위치 민감 정보 추가
    - 모델 학습 과정에서 함께 학습됨
  - `Transformer encoder`: 패치 처리
  - `MLP Head`를 통해 분류 작업 수행
    - Head를 수정하여 다른 작업을 위한 `transfer learning` 활용 가능
    - CLIP에서는 `CLIP 학습법`으로 학습됨
- 입력 구성
  - 텍스트 인코더 (자연어 데이터 입력) : `Sub-word` 단위의 임베딩
  - 이미지 인코더 (이미지 데이터 입력) : `패치` 단위의 임베딩
  - ViT : 비전 분야에 트랜스포머를 (최소 수정으로) 적용한 모델

#### CLIP (2021) 학습
- 대조 학습(Contrastive learning)
  - 학습 기준
    - 목표 이미지(`앵커`)를 대응하는 텍스트(`양성`)와 `가깝게`
    - 일치하지 않는 여러 텍스트(`음성`)와는 `멀게`

#### CLIP 간단 응용
- 제로샷 이미지 인식
  - 텍스트로 원하는 물체 `카테고리 리스트` 준비
  - 텍스트 기반 카테고리 리스트를 텍스트 임베딩으로 변환하여 `Vector DB` 준비
  - 쿼리 이미지와 비교해서 `가장 높은 점수`의 카테고리 반환
- 생각해보기
  - 검색 시스템과 유사성은 무엇일까?
  - 카테고리 이외에 어떤 것이 가능할까?
  - 카테고리가 정말 많을 경우에 어떻게 효율화 할까?

## VLM(Bision-Language Model)
### CLIP
#### 멀티모달 정합(Multi-modal Alignment)
- 서로 다른 두 가지 이상의 모달리티(예 : 이미지와 텍스트) 간의 공통된 임베딩 벡터 공간을 구성하는 것
- 서로 다른 모달리티 임베딩 간 유사도(연관성) 비교 가능
- 대표적인 모델:
  - `CLIP(OpenAI)`: 이미지와 텍스트 간의 Multi-modal Alignment를 효과적으로 수행
  - `ImageBind(Meta)`: 더 다양한 모달리티(예: 소리, 텍스트, 이미지, 열화상, 깊이)를 결합

#### ImageBIND - One Embedding Space To Bind Them All
- 이미지, 비디오, 텍스트, 오디오, 뎁스, 열화상, IMU 모달리티 공간을 공유하도록 학습

#### LLaVA(Large Language and Vision Assistant: 2023)
- Vision과 Language모델을 결합한 모델(VLM)로, 텍스트와 이미지를 동시에 이해
- 주요 특징
  - 이미지 인식과 텍스트 생성을 결합하여, 이미지 설명 생성 또는 시각적 질문 응답 작업에서 뛰어난 성능
  - 이미지, 명령(Instruction), 답변이 주어진 데이터셋을 구축하여 `Instruction tuning` 으로 학습
- 응용 사례
  - 이미지 기반 질문 응답(Visual QA), 이미지 설명 생성, 시각적 정보 기반 대화 등
- LLaVA 모델 특징
  - `효율적인 메모리 사용`: 적은 자원으로 큰 모델을 효과적으로 학습
  - `다중 모달 학습`: 텍스트와 시각 데이터를 결합하여 응답을 생성
  - `Fine-tuning`: 특정 작업에 맞춰 모델을 미세조정하여 사용

#### SigLIP (2023)
- CLIP 대비 SigLIP이 압도적인 성능을 보이며 최신 VLM에 널리 활용됨 (최근 SigLIP 2도 공개)
- SigLIP은 softmax 대신 sigmoid 기반 손실함수
  - 기존 CLIP에서 사용한 대조학습 (Contrastive learning)의 한계는 무엇일까?
    - 어느 정도 이미 멀게 배치한 음성 데이터들에 대해서도 계속 거리를 벌리기 위해 학습이 진행됨
  - SigCLIP : CLIP과 달리 일치하지 않는 음성 데이터에 제한된 영향만 받도록 손실함수 디자인을 고침

#### VLM의 눈으로 응용
- CLIP 기반 VLM들 리스트
  - `BLIP-2` - CLIP + OPT/FlanT5 결합
  - `InstructBLIP` - BLIP-2의 instruction tuning 버전
  - `LLaVA` - CLIP + Vicuna
  - `MiniGPT-4` - CLIP + Vicuna 기반
  - `mPLUG-Owl` - CLIP 기반 Alibaba의 VLM
- SigLIP 기반 VLM들 리스트
  - `PaLI-X` - SigLIP + PaLM 결합
  - `SmolVLM`
- 최근에는 CLIP, SigLIP의 성공적인 레시피를 기반으로 특화 Vision encoder 모델들이 개발되는 추세

