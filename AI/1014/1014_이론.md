## AI & 기계학습 방법론 1
### 선형 회귀
#### 선형회귀(Linear Regression)
  - 선형회귀란?
    - 입력 변수와 출력 변수 사이의 관계를 `직선 형태`로 `근사`하여, 예측하는 통계적 방법이다.
    - 지도학습의 가장 `기초`가 되는 접근 중 하나이다.
    - 단순해 보이지만, 선형회귀는 `개념적으로도`, `실무적으로도` 매우 유용하다.
- ![선형회귀](수업자료/1-1%20선형회귀.png)

#### 광고데이터 예시
- 선형회귀를 통해 대답할 수 있는 질문들
  - 광고비와 매출 사이에 관계가 있는가?
  - 그 관계의 강도는 어느 정도인가?
  - 어떤 매체가 매출에 기여하는가?
  - 미래 매출을 얼마나 정확히 예측할 수 있는가?
  - 매체 간에 상호작용(시너지)가 있는가?

### 단순선형회귀
#### 단순선형회귀: 단일 설명변수를 이용한 선형회귀
- 단순선형회귀(simple linear regression)란?
  - 한 개의 설명변수(X)와 하나의 반응변수(Y) 사이의 선형(직선) 관계를 찾는 방법
  - 목표: 데이터를 가장 잘 설명하는 직선을 찾아 예측(ŷ)에 활용
- 단일 설명변수를 이용한 단순선형회귀
  - 모형 가정: $Y = \beta_0 + \beta_1X + \epsilon$
    - $\beta_0$: 절편 ($X = 0$일 때 Y 값)
    - $\beta_1$: 기울기 ($X$가 1단위 증가할 때 Y의 평균 증가량)
    - $\epsilon$: 관측 오차
- hat(예, $\hat{y}, \hat{\beta}$) 표기는 추정값을 의미
- ![단순선형회귀](수업자료/2-1%20단순선형회귀.png)

#### 최소제곱법(least squares)
- 최소제곱법(least squares)이란?
  - 실제 관측값과 예측값의 차이(잔차, residual)를 제곱해 합한 값(RSS, `잔차제곱합`)을 최소화하는 방법
  - 목표: 데이터를 가장 잘 설명하는 직선을 찾기 위해 계수 $\beta_0, \beta_1$을 추정
- 잔차(residual) 정의: $e_i = y_i - \hat{y}_i$ (예측값 $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$)
- RSS(잔차제곱합) 정의: $RSS = e_1^2 + e_2^2 + \cdots + e_n^2$
- 다른 표현 RSS = $\sum_{i=1}^n e_i^2 = \sum_{i=1}^n(y_i - \hat{y}_i)^2 = \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1x_i)^2$
- 계수를 추정하기 위한 공식: closed-form solution(공식으로 바로 계산할 수 있는 해) 존재함!
  - 기울기 $\hat{\beta}_1$
    $$ \hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} $$
  - 절편 $\hat{\beta}_0$
    $$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} $$
  - 참고: $\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i, \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$

#### 단순선형회귀: 광고 데이터
- 사례 설명
  - 목표: TV 광고비(X)과 제품 판매량(Y)의 선형 관계 예측
  - 단순선형회귀를 적용하여, 각 데이터에서 잔차제곱을 가장 작게 만드는(최소제곱법)이 선택됨
- 도형의 의미
  - `파란 직선`: 최소제곱법으로 계산한 회귀선
  - `빨간 점`: 실제 관측 데이터
  - `회색 세로선`(빨간 점에서 파란 선까지) = 잔차(residual)
  - RSS(잔차제곱합)를 최소화할 때, 최적의 $\hat{\beta}_0, \hat{\beta}_1$이 결정됨
- 수식
  - 기울기 $\hat{\beta}_1$
    $$ \hat{\beta}_1 = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2} $$
  - 절편 $\hat{\beta}_0$
    $$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} $$
- ![단순선형회귀 결과](수업자료/2-2%20광고비와%20매출의%20단순성형회귀%20결과.png)

- 단순선형회귀 결과 해석 (광고 데이터)
  - 계수 해석
    - 절편(Intercept) = 7.03 → TV 광고비가 0이어도 기본적으로 평균 판매량은 7.03백만원
    - TV 광고비 계수 = 0.0475 → TV 광고비를 1단위(1백만원) 늘리면 평균 매출이 약 0.0475x1단위(1백만원) = 4.72만원 증가
- [계수 해석](수업자료/2-3%20광고%20데이터%20해석.png)
- 유의성 검정
  - 계수의 p-value < 0.0001 (매우 작음, <<0.05) 이므로 통계적으로 매우 유의함 → `TV 광고비와 매출 간 관계 존재`
- 모형 적합도($R^2$ 높을 수록, 1에 가까울 수록 좋음)
  - $R^2$=0.612 → 판매량 변동의 약 61%를 광고비로 설명 가능
- [유의성 검정](수업자료/1-1%20선형회귀.png)

### 다중선형회귀
#### 다중선형회귀(multiple linear regression)란?
- 단순선형회귀와 다중선형회귀
  - 단순 선형 회귀: "TV광고 → 매출" 한 가지 관계만 고려
  - 다중 선형 회귀: "TV광고, Radio 광고비, 가격, 계절, 경쟁사" 등 복수 요인을 함께 고려하여 매출을 설명
- ![단순선형회귀](수업자료/3-1%20다중선형회귀.png)
- ![다중선형회귀](수업자료/3-1%20단순선형회귀.png)
- 다중선형회귀의 개념
  - 독립 변수(설명 변수, Feature)가 여러 개 존재할 때 사용하는 회귀 분석 기법
  - 단순 선형 회귀는 하나의 변수만 고려하지만, 다중 선형 회귀는 여러 독립 변수($X_1, X_2, \dots$)를 동시에 고려하여, 종속 변수(Y)와의 관계를 구함
- ![다중선형회귀 수식](수업자료/3-1%20다중선형회귀%20수식.png)
- 각 변수의 의미
  - Y: 종속 변수 (예측 대상, 예: 매출)
  - $X_1, X_2, \dots, X_p$: p개의 독립 변수들 (예: 광고비, 가격, 계절 등)
  - $\beta_0$: 절편
  - $\beta_1, \beta_2, \dots, \beta_p$: 각 독립 변수에 대한 회귀 계수(모수) (`변수의 영향력 크기와 방향을 나타냄`)
  - $\epsilon$: 관측 오차 (모델이 설명하지 못하는 부분, 오류/잔차가 아님!)

$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon $$

  - 해석: `다른 변수를 고정한 채` $X_j$가 1단위 증가할 때 Y가 평균적으로 $\beta_j$만큼 변화
  - 광고 데이터 예: $sales = \beta_0 + \beta_1TV + \beta_2radio + \beta_3newspaper$

#### 다중선형회귀의 추정과 예측
- 다중선형회귀: 계수 추정과 예측
  - 여러 변수($x_{i1}, x_{i2}, \dots, x_{ip}$)로 반응 변수 Y를 동시에 예측하는 모형
  - 예측값: $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_{i1} + \hat{\beta}_2x_{i2} + \cdots + \hat{\beta}_px_{ip}$
  - 추정 방법: 실제 값과 예측 값의 차이(잔차, $e_i = y_i - \hat{y}_i$)를 제곱해 합한 값(RSS)를 최소화
  - RSS가 최소일 때 얻어지는 계수 $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_p$가 최적 추정치
  $$ RSS = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1x_{i1} - \cdots - \hat{\beta}_px_{ip})^2 $$
  - 여러 입력 변수를 동시에 고려하여 데이터와 가장 가까운 평면(hyperplane)을 찾는 과정

#### 다중선형회귀 계수 추정 유도(행렬 표현)
- 다중선형회귀: 계수 추정의 수학적 유도
  - 행렬 표현
$$ \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} = \begin{bmatrix} 1 & x_{11} & \cdots & x_{1p} \\ 1 & x_{21} & \cdots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & \cdots & x_{np} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix} = \mathbf{X}\beta + \epsilon $$
  - 최소제곱법 목적: RSS 최소화
$$ RSS = (\mathbf{y} - \mathbf{X}\hat{\beta})^\mathrm{T}(\mathbf{y} - \mathbf{X}\hat{\beta}) = \mathbf{y}^\mathrm{T}\mathbf{y} - \hat{\beta}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{y} - \mathbf{y}^\mathrm{T}\mathbf{X}\hat{\beta} + \hat{\beta}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}\hat{\beta} $$
$$ \xrightarrow{\text{미분}} \frac{\partial RSS}{\partial \hat{\beta}} = -2\mathbf{X}^\mathrm{T}\mathbf{y} + 2\mathbf{X}^\mathrm{T}\mathbf{X}\hat{\beta} = 0 $$
  - 정규방정식 해
$$ \hat{\beta} = [\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_p]^\mathrm{T} = (\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\mathbf{X}^\mathrm{T}\mathbf{y} $$

#### 다중선형회귀 결과: 시각화
- ![다중선형회귀 결과](수업자료/3-4%20광고비와%20매출%20다중선형회귀%20결과.png)

#### 다중선형회귀 결과: 광고 데이터
- 다중선형회귀 결과 해석 (광고 데이터)
  - TV, 라디오 광고비는 매출 증가에 유의미한 관계를 갖음(각 p-value들이 매우 낮음 << 0.05)
  - 신문 광고비는 통계적으로 유의하지 않음(p-value = 0.8599 > 0.05) → 매출과 관계가 거의 없음

| | Coefficient | Std. Error | t-statistic | p-value |
|---|---|---|---|---|
| Intercept | 2.939 | 0.3119 | 9.42 | < 0.0001 |
| TV | $\hat{\beta}_1 = 0.046$ | 0.0014 | 32.81 | < 0.0001 |
| radio | $\hat{\beta}_2 = 0.189$ | 0.0086 | 21.89 | < 0.0001 |
| newspaper | $\hat{\beta}_3 = -0.001$ | 0.0059 | -0.18 | 0.8599 |
- 모형 적합도($R^2$ 높을 수록, 1에 가까울 수록 좋음)
  - 결정계수 $R^2 = 0.897$: 모델 설명력이 매우 높음 → 단순선형회귀 결과와 비교하였을 때 향상된 예측

| Quantity | Value |
|---|---|
| $R^2$ | 0.897 |

### 선형회귀 주의사항
#### 검증/테스트셋 데이터를 활용한 성능 평가
- 선형회귀 결과 검증 및 테스트 성능
  - 훈련 데이터에서의 성능
    - 회귀식을 만들 때 최소제곱 해는 훈련 데이터만 보고 계산됨
    - 학습에 사용된 훈련 데이터에서는(X과 Y의 평균적인 선형관계가 있다면) 적합(fitting)이 잘 되어 있을 것임.
    - 그러나 이것은 테스트 성능을 과소평가할 가능성이 높음
  - 테스트 성능 평가 필요
    - 선형회귀도 변일반화 성능을 확인하려면 훈련에 사용되지 않은 새로운(테스트) 데이터에 적용해 봐야 함
    - 수가 많거나 고차항을 사용하면 `과적합(overfitting) 문제`가 여전히 발생할 수 있음.
    - 검증/교차검증을 통해서 적절한 적합을 찾을 수 있음.

#### 다중선형회귀 회귀계수 해석의 주의점
- 회귀계수 해석 시 주의점
  - 이상적 상황: 변수들이 연관(correlation)되지 않고, 독립적일 때 → 계수 해석이 명확함
  - 문제 상황: 변수들이 서로 연관되어 있다면 → 계수 추정이 불안정해지고 해석에 혼동이 발생할 수 있음
  - 주의: 관찰 데이터의 상관관계로 인과 관계를 주장해서는 안 됨.
    - 예로 들었던, 광고 데이터는 자연스럽게 인과성이 있는 것 처럼 보이지만, 다른 많은 데이터에서 선형 관계를 보인다고 하여 인과 관계가 있는 것이 아님.
    - 예: "아이스크림 소비량"(X) vs "상어에 물리는 사건"(Y)

### 요약 및 정리
- 선형회귀 개요
  - 지도학습의 가장 기초적인 방법
  - 입력과 출력의 선형 관계를 학습
- 단순선형회귀
  - 모형: $Y = \beta_0 + \beta_1X + \epsilon$
  - 의미: $\beta_0$: 절편, $\beta_1$: 기울기, $\epsilon$: 측정오차
  - 학습 방법: 최소제곱법 (RSS 최소화)
  - 사례: TV 광고비와 매출 관계
- 다중선형회귀
  - 모형: $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon$
  - 해석: 다른 변수 고정 시, 특정 $X_j$가 1 증가 → Y는 평균적으로 $\beta_j$ 변화
  - 사례: TV + Radio + Newspaper 광고비와 매출
- 계수 추정과 예측
  - RSS 최소화를 통해 계수 추정
  - 해: 최소제곱 => 정규방정식 사용
  - 결과는 평면(hyperplane)으로 표현
- 선형회귀 주의사항
  - 훈련 데이터 기반 계산 → 검증/테스트셋 필요
  - 다중공선성 문제 발생 가능 (변수 간 높은 상관성)
    - 계수 해석 어려움
    - 분산 증가 → 불안정한 추정

## AI & 기계학습 방법론 2
### 분류
#### 분류(Classification)란?
- 분류의 정의
  - 분류: 정해진 범주(카테고리) 중 하나로 지정하는 것.
  - 범주형 변수: 수치의 크고 작음이 아니라 유한한 범주(성별, 혈액형, 지역 등)로 표현하는 변수
    - 예: 눈동자 색 {black, brown, blue, green}, 이메일 종류 {spam, normal}
- 분류 함수의 목표
  - 입력 (X, 특성 벡터) : 눈동자 이미지, 이메일 내용/제목
  - 출력 (Y ∈ C, 범주) : 예, 눈동자 색 C = {black, brown, blue, green}, 이메일 C = {spam, normal}, 순서 없는 집합
  - 분류 함수의 목표
    - 분류 함수 f(X)를 학습하여 입력 X가 속할 범주(카테고리)를 예측
    - 범주의 직접 예측보다 각 범주에 속할 확률 P(Y = k | X)를 추정하는 것이 더 유용할 때가 많음

#### 예시: 신용카드 연체(Default)
- 그룹별 분포(Boxplot)
  - 신용카드 사용량(Balance), 소득(Income)에 대해 연체(Default)의 그룹별 분포(Boxplot).
  - 신용카드 사용량과 소득은 각각 독립변수, 연체 여부가 종속 변수
- 그룹별 분포 해석
  - 신용카드 사용량(Balance) vs 연체(Default)
    - `연체자`의 신용카드 사용량이 `연체하지 않은 사람`보다 전반적으로 높음.
    - 중앙값도 높고, 분포가 퍼져 있는 정도도 더 큼.
  - 소득(Income) vs 연체(Default)
    - 연체 여부에 따른 소득 차이는 거의 없음.
    - 중앙값이 약간 다르지만 분포가 대부분 겹침.
- 신용카드 사용량-소득 산점도 해석
  - `연체자(주황색 +)`는 신용카드 사용량이 높은 구간에 집중적으로 분포
  - `연체가 없는 사람(파란색 O)`은 신용카드 사용량이 낮은 쪽에 주로 분포
  - 소득은 연체 여부와 뚜렷한 상관이 보이지 않음.
- ![신용카드 소득 산점도](수업자료2/1-2%20신용카드%20소득%20산점도.png)
- ![그룹별 분포 해석](수업자료2/1-2%20그룹별%20분포%20해석.png)

#### 분류 문제에 선형회귀를 써도 될까?
- 선형회귀는 분류 문제에 사용하기에 부적절함: 이진 분류 문제
  - 선형회귀는 선형함수를 계산하는 문제로 예측 값이(Y값 기준) 제한된 값을 갖지 못함.
  - 따라서 선형회귀는 `예측 확률이 0보다 작거나 1보다 크게 예측` 될 수 있어 확률로 쓰기 부적절함.
    - 예: 응급실 환자 진단 {0: 비응급, 1: 응급}
    - 선형회귀는 확률 범위(0~1)를 벗어나는 값을 내놓을 수 있어 문제 발생.
- ![응급실 환자 진단1](수업자료2/1-3%20응급실%20환자%20진단.png)
- 다중 범주 분류 문제
  - 선형회귀는 정수형 코딩(1, 2, 3)에 따라 범주 간 순서와 동일한 거리를 가정.
  - 범주(카테고리) 변수는 순서가 없는 라벨이므로 부적절함.
    - 예: 응급실 환자 진단 {1: 뇌졸중, 2: 약물과다복용, 3: 간질발작}
    - 실제로는 범주 간 순서나 거리가 존재하지 않음.
  - 따라서, 선형회귀는 분류 문제에 부적절함
- ![응급실 환자 진단2](수업자료2/1-3%20응급실%20환자%20진단%202.png)

#### 분류 문제에서 적합한 모델
- 분류 문제에서 선형회귀의 대안 – 로지스틱 회귀(Logistic Regression)
  - 시그모이드(Sigmoid) 함수를 활용해 0~1 범위 내 확률값 예측 보장
  - 순서가 없는 범주를 확률로 직접 예측하는 적절한 분류 방법
- ![시그모이드 함수](수업자료2/1-4%20시그모이드%20함수.png)

### 로지스틱 회귀
#### 로지스틱(Logistic)회귀의 모형식
- 이진 분류에서의 적절한 함수
  - 함수의 출력 범위가 모든 입력에 대해 0~1 사이의 범위를 가지는 함수를 활용하자.
- 출력 범위가 0~1인 시그모이드(Sigmoid) 함수
  - 기울어진 S자 형태의 곡선 함수
    $$ y = \frac{e^z}{1+e^z} = \frac{1}{1+e^{-z}} $$
    - (오일러 수 $e \approx 2.71828$)
  - 모든 실수 z에 대한 y값의 범위는 $0 \le y \le 1$이다.
    - $z \to +\infty$ 일때, y는 1로 수렴
    - $z \to -\infty$ 일때, y는 0으로 수렴
- ![로지스틱 회귀](수업자료2/2-1%20로지스틱회귀%20모형식.png)
- 로지스틱 회귀(Logistic Regression)의 모형식
  - 시그모이드(Sigmoid) 함수 $y = \frac{e^z}{1+e^z}$ 의 z에
  - (Linear Regression) 식 $z = \beta_0 + \beta_1x$ 식을 대입하면,
    $$ y = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} $$
  - 확률 표기 $p(x)$를 활용한다면, 로지스틱 회귀의 모형식이 된다.
    - $p(X) = P(Y=1|X)$ 로 두고, (X가 주어졌을 때, 결과가 1일 확률)
    $$ p(X; \beta) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} = \frac{1}{1+e^{-(\beta_0+\beta_1x)}}, \quad \beta=[\beta_0, \beta_1]^\mathrm{T} $$
- [로지스틱 회귀](수업자료2/2-1%20로지스틱회귀%20모형식2.png)

#### 로지스틱 회귀 모형 vs 선형 모형
- 로지스틱 함수와 선형회귀의 관계
  - 로지스틱 함수 $p(x; \beta) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$는 선형 함수 $y = \beta_0 + \beta_1x$를 내포하고 있다.
  - 이 두 함수 간의 관계를 해석하기 위해 오즈(Odds)와 로짓 변환(logit)에 대해 알아보자
- 오즈(Odds)
  - 오즈(Odds)란, 성공(y=1)확률이 실패(y=0)확률에 비해 몇 배 더 높은가를 나타낸다.
    $$ \text{odds} = \frac{p(y=1|x)}{p(y=0|x)} = \frac{p(y=1|x)}{1-p(y=1|x)} = \frac{\text{성공확률}}{\text{실패확률}} $$
- 로짓 변환(logit) = Log odds
  - 로짓변환은 오즈(odds)에 로그를 취한 함수 형태
    $$ \text{logit}(p) = \log(\text{odds}) = \log\frac{p(y=1|x)}{1-p(y=1|x)} $$
- 로지스틱 함수 모형식 $p(y=1|x) = p(x; \beta) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$ 에 로짓 변환을 수행해보자
  $$ \text{logit}(p) = \log(\text{odds}) = \log\frac{p(y=1|x)}{1-p(y=1|x)} = \log\frac{p(x;\beta)}{1-p(x;\beta)} = \beta_0 + \beta_1x $$
- 즉, 로지스틱 모형식은 `선형 모형식`과 `시그모이드(sigmoid) 함수`의 결합이며, `로짓 변환`시 선형 회귀 모형식으로 표현이 가능하다.

`로지스틱 모형 = 선형 모형 + 시그모이드 함수`
$$ p(x;\beta) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} \quad \Longleftrightarrow \quad \log\frac{p(x;\beta)}{1-p(x;\beta)} = \beta_0+\beta_1x $$
`로지스틱 모형의 로짓 변환 = 선형 모형`

#### MLE 활용 모수 추정
- 우도(Likelihood)
  - 확률을 추정하는 모델을 결정했으니, 모수($\beta_0, \beta_1$)를 추정하는 방법에 대해 알아보자.
  - 선형회귀에서 "현재 함수가 데이터와 오차가 작은지"를 평가하기위해 평균 제곱 오차(MSE: Mean Squared Error)를 지표로 삼았듯, 확률을 계산하는 함수를 평가하기 위해선 우도(Likelihood)를 지표로 삼는다.
  - 우도란 "현재 확률 함수가 데이터를 얼마나 잘 설명하는지"를 나타낸 지표이다.
  - 그러므로, 모델의 학습은 우도 값을 높여 최대화가 되도록 하는 것이 목표이며 이를 `Maximum Likelihood Estimation (MLE)`이라 한다.
- 로지스틱 회귀의 우도 최대화
  - 로지스틱 회귀와 같은 이진 분류 문제에서 우도를 최대화한다.
    $$ \underset{\beta}{\text{maximize}} \ \mathcal{L}(\beta) = \prod_{i:y_i=1} p(x_i; \beta) \prod_{i':y_{i'}=0} (1-p(x_{i'}; \beta)) $$
  - 하지만, 위와 같은 곱으로 이뤄진 함수의 경우, $\beta$에 대해 미분이 어렵기 때문에 양 변에 log를 취해 곱셈을 더하기로 변환시킨 뒤 log-likelihood를 만들어 최대화한다.
    $$ \underset{\beta}{\text{maximize}} \ \log\mathcal{L}(\beta) = \sum_{i=1}^n [y_i\log p(x_i;\beta) + (1-y_i)\log(1-p(x_i;\beta))] $$
  - $\log\mathcal{L}(\beta)$를 미분하여 `도함수=0`에 근접하도록 수치적(반복) 최적화를 통해 $\beta$들을 찾아 나간다.
- log-likelihood로 변형하여 최대화하는 $\beta$를 구해도 되는가?
  - 로지스틱 회귀에서 우도(likelihood)를 최대화하는 것인데, log변환을 한 log-likelihood를 최대화 하는 것으로 변형하여 문제에 접근해도 과연 괜찮을까?
  - Log함수는 단조(monotone) 증가 함수임.
  - 따라서, `log-likelihood를 최대화하는 $\beta$`와 `likelihood를 최대화하는 $\beta$`는 같다!
- ![우도 최대화](수업자료2/2-3%20우도%20최대화.png)

#### 로지스틱 회귀 결과: 신용카드 연체 데이터
- 신용카드 연체 데이터 결과 해석
  - 신용카드 사용량(Balance)과 연체 여부(Default)로 로지스틱 회귀 모형을 학습한 결과.

| | Coefficient | Std. Error | z-statistic | p-value |
|---|---|---|---|---|
| Intercept | -10.6513 | 0.3612 | -29.5 | <0.0001 |
| balance | 0.0055 | 0.0002 | 24.9 | <0.0001 |
- ![신용카드 연체 데이터](수업자료2/2-4%20로지스틱%20회귀%20결과%20해석.png)
  - 추정 결과 $\hat{\beta}_1 = 0.0055$
  - 즉 신용카드 사용량(Balance)이 1단위 증가할 때 연체(Default)의 로짓(log-odds)가 0.0055 증가.
    $$ \log\frac{p(X;\hat{\beta})}{1-p(X;\hat{\beta})} = -10.6513 + 0.0055 \cdot X $$
- 신용카드 사용량 이외에도 다양한 입력(소득, 학생 여부 등)을 추가하여 여러 변수 $X_1, \dots, X_p$를 함께 모형에 사용하면 회귀 계수를 통해 연체할 확률을 계산할 수 있음.
  $$ \log\frac{p(X;\beta)}{1-p(X;\beta)} = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p $$
  $$ p(X;\beta) = \frac{e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}} $$
| | Coefficient | Std. Error | z-statistic | p-value |
|---|---|---|---|---|
| Intercept | -10.8690 | 0.4923 | -22.08 | <0.0001 |
| balance | 0.0057 | 0.0002 | 24.74 | <0.0001 |
| income | 0.0030 | 0.0082 | 0.37 | 0.7115 |
| student [Yes] | -0.6468 | 0.2362 | -2.74 | 0.0062 |

### 요약 및 정리
- 분류 문제란?
  - 분류 : 정해진 범주(카테고리) 중 하나로 지정하는 것
  - 분류 모델의 목표 : 분류 함수 f(X)를 학습하여 입력 X가 속할 범주(또는 그 확률)를 예측
- 분류 문제에서 선형회귀 모델 적용의 한계
  - 이진 분류 문제 : 선형회귀는 예측 값이 확률 범위(0~1)를 벗어나 부적합
  - 다중 분류 문제 : 정수로 코딩 시 인위적 순서를 암시하기에 부적합
- 로지스틱 회귀(Logistic Regression)는 어떻게 작동할까?
  - 로지스틱 함수 모형식은 선형 회귀 모형식과 sigmoid 함수의 결합
  - 우도 : "확률함수가 데이터를 얼마나 잘 설명하는지"를 나타낸 지표
  - 최대 우도 추정(MLE: Maximum Likelihood Estimation) : 우도가 최대가 되는 모수를 찾는 과정

## AI & 기계학습 방법론 3
### 모수적 함수로서의 선형 모델
#### 단순(1D) 선형모델: 모수적 함수
- ![모수적 함수](수업자료3/1-1%20모수적%20함수.png)

#### 단순(1D) 선형모델 학습
- ![단순 선형모델 학습](수업자료3/1-2%20단순%20선형모델%20학습.png)

### Shallow 네트워크
#### Shallow 네트워크 vs 1D 선형회귀
- ![1D 선형회귀 vs Shallow 네트워크](수업자료3/2-1%20shallow%20네트워크%20vs%201D%20선형회귀.png)

#### Shallow 네트워크: 활성화 함수
- ![활성화 함수](수업자료3/2-2%20활성화%20함수.png)

#### Shallow 네트워크: 모수(parameter)
$$ y = f[x, \phi] = \phi_0 + \phi_1a[\theta_{10} + \theta_{11}x] + \phi_2a[\theta_{20} + \theta_{21}x] + \phi_3a[\theta_{30} + \theta_{31}x] $$
- 위 네트워크 모델은 10개의 모수를 갖고 있음
  $$ \phi = \{\phi_0, \phi_1, \phi_2, \phi_3, \theta_{10}, \theta_{11}, \theta_{20}, \theta_{21}, \theta_{30}, \theta_{31}\} $$
- 모수($\phi$)가 정해지면 특정 함수(f[x, $\phi$])가 결정된다.
- 모수가 주어지면 추론(식을 계산, 예측)할 수 있다.
- 훈련데이터 $\{(x_i, y_i)\}_{i=1}^l$가 주어지면 `손실함수`(예, least squares)를 정의하고 손실을 최소화하도록 모수를 조정한다.

#### Shallow 네트워크: piecewise linear 함수
- 입력 구간을 나눠 `조각별 선형` 함수를 만듦
  $$ y = f[x, \phi] = \phi_0 + \phi_1a[\theta_{10} + \theta_{11}x] + \phi_2a[\theta_{20} + \theta_{21}x] + \phi_3a[\theta_{30} + \theta_{31}x] $$
- ![piecewise linear 함수](수업자료3/2-4%20piecewise%20linear.png)
- 다른 모수($\phi$)에 따라서 달라지는 함수(f[x, $\phi$])들의 예 a), b), c)

#### Shallow 네트워크: Hidden Units
$$ y = f[x, \phi] = \phi_0 + \phi_1a[\theta_{10} + \theta_{11}x] + \phi_2a[\theta_{20} + \theta_{21}x] + \phi_3a[\theta_{30} + \theta_{31}x] $$
- 두 부분으로 나눠 생각:
  $$ y = \phi_0 + \phi_1h_1 + \phi_2h_2 + \phi_3h_3 $$
  - Hidden units
  $$ \begin{cases} h_1 = a[\theta_{10} + \theta_{11}x] \\ h_2 = a[\theta_{20} + \theta_{21}x] \\ h_3 = a[\theta_{30} + \theta_{31}x] \end{cases} $$

#### Shallow 네트워크: 각 단계별 계산
- ![단계별 계산1](수업자료3/2-6%20선형%20변환,%20활성화%20함수%20적용.png)
- ![단계별 계산2](수업자료3/2-6%20활성화%20후.png)

#### 네트워크 도식화
- ![네트워크 도식화](수업자료3/2-7%20네트워크%20도식화.png)

### Shallow 네트워크의 표현력
#### 더 많은 Hidden Unit 가능
- 3개의 hidden units:
  - $h_1 = a[\theta_{10} + \theta_{11}x]$
  - $h_2 = a[\theta_{20} + \theta_{21}x]$
  - $h_3 = a[\theta_{30} + \theta_{31}x]$
  - $y = \phi_0 + \phi_1h_1 + \phi_2h_2 + \phi_3h_3$
- (일반적인) D개의 hidden units:
  - $h_d = a[\theta_{d0} + \theta_{d1}x]$
  - $y = \phi_0 + \sum_{d=1}^D \phi_dh_d$

#### Hidden Unit을 많이 두면
- 충분히 많은 Hidden Unit이 있다면, 임의의 1차원 함수를 `원하는 정확도`로 근사할 수 있다.
- ![hidden unit](수업자료3/3-2%20hidden%20unit.png)

#### 보편적 근사 정리(Universal approximation theorem)
- "Hidden unit을 `충분히 많이` 갖는다면, 얕은 신경망은 임의의 연속함수를 임의의 정밀도로 근사할 수 있음"

### 다중 출력/입력
#### 임의의 개수의 입력, Hidden Unit, 출력
- ![임의의 개수의 입력](수업자료3/4-1%20임의의%20개수의%20입력.png)

### Deep 네트워크
#### 2개의 네트워크를 하나로 합성
- ![2개의 네트워크](수업자료3/5-1%202개의%20네트워크.png)

## Ai & 기계학습 방법론 4
### 선형회귀 예시
#### 손실함수
- 손실함수
  - 학습 데이터셋: input/output의 I개 쌍
    - 학습 데이터셋 표기: $\{ \mathbf{x}_i, y_i \}_{i=1}^I$
  - 손실함수(Loss function): 모델이 얼마나 잘못 예측하는지를 측정하는 함수
    - 값이 작을수록 모델이 더 정확하게 학습되었다는 의미
  - 손실함수 정의: $L[\phi, f[ \mathbf{x}_i, \phi], \{ \mathbf{x}_i, y_i \}_{i=1}^I ]$
  - 손실함수의 간단한 표현: $L[\phi]$ ← 입력을 출력에 더 잘 매핑할수록 더 작은 값을 반환하는 스칼라를 돌려줌

#### 학습
- 손실함수
  - 학습: 손실함수를 최소화하는 파라미터를 찾음
    $$ \hat{\phi} = \underset{\phi}{\text{argmin}} [L[\phi]] $$
    - $\hat{\phi}$: 모델의 파라미터(우리가 조정해야하는 값) 찾기
    - $\underset{\phi}{\text{argmin}} [L[\phi]]$: 손실함수 $L[\phi]$를 가장 작게 만드는 파라미터 찾기

#### 1D 선형회귀 학습
- 경사 하강법(gradient descent)
  - 손실 함수의 값이 줄어드는 방향으로 파라미터를 이동하는 과정
- 선형회귀 학습
  - 등고선(왼쪽 그림): 손실 함수 값의 크기
    - 밝을수록 손실이 큼
    - 어두울수록 손실이 적음
  - 데이터와 선형함수 직선(초록색 선): 데이터와 선의 오차가 크다면 손실 값이 큼
    - 주황색 점: 실제 데이터
    - 초록색 직선: 현재 파라미터로 만든 모델
- ![데이터와 직선](수업자료4/1-1%20데이터와%20직선.png)

#### 1D 선형회귀 학습
- 경사 하강법(gradient descent)
  - 손실 함수의 값이 줄어드는 방향으로 파라미터를 이동하는 과정
- 선형회귀 학습
  - 등고선(왼쪽 그림): 손실 함수 값의 크기
    - 밝을수록 손실이 큼
    - 어두울수록 손실이 적음
  - 데이터와 선형함수 직선(초록색 선): 데이터와 선의 오차가 크다면 손실 값이 큼
    - 주황색 점: 실제 데이터
    - 초록색 직선: 현재 파라미터로 만든 모델
- ![등고선 그림](수업자료4/1-2%20등고선%20그림.png)

### 수학 리뷰
#### 미분을 이용한 최적화
- 미분을 통한 기울기 이해
  - 이차 함수로, 그래프는 위로 열린 포물선
  - 미분 결과(2x-4)는 기울기를 의미
    - 기울기가 0이 되는 지점이 극값(최소값)
- ![포물선](수업자료4/2-1%20포물선%20함수와%20기울기.png)

### 경사 하강
#### 경사 하강법
- 경사 하강(Gradient descent) 알고리즘
  - 경사 하강법은 손실 함수 $L[\phi]$를 최소화하기 위해 파라미터 $\phi$를 반복적으로 갱신하는 알고리즘
  - 기울기 계산: 손실 함수 $L[\phi]$를 파라미터 $\phi$에 대해 편미분(각 원소에 대한 미분) 진행
    - 벡터 형태의 기울기
      $$ \frac{\partial L}{\partial \phi} = \begin{bmatrix} \frac{\partial L}{\partial \phi_0} \\ \frac{\partial L}{\partial \phi_1} \\ \vdots \\ \frac{\partial L}{\partial \phi_N} \end{bmatrix} $$
  - 파라미터 업데이트
    - 기울기(미분값)의 `반대 방향`으로 이동해야 손실 함수가 줄어듬
      $$ \phi \longleftarrow \phi - \alpha \frac{\partial L}{\partial \phi} $$
    - 여기서 $\alpha \ge 0$는 학습률(learning rate)로, 한 번의 스텝에서 이동하는 크기를 결정
- 손실 함수의 기울기 계산
  - 경사 하강법의 첫 단계: 편미분(전체 손실에 대한 기울기) 구하기
    - 손실 함수: 여기서 $l_i$는 각 데이터 샘플 $i$의 손실 값
      $$ L[\phi] = \sum_{i=1}^I l_i = \sum_{i=1}^I (f[x_i, \phi] - y_i)^2 = \sum_{i=1}^I (\phi_0 + \phi_1x_i - y_i)^2 $$
    - 전체 손실 $L[\phi]$은 모든 데이터 샘플 손실 $l_i$의 합
    - 전체 기울기는 각 데이터의 기울기를 합한 것
      $$ \frac{\partial L}{\partial \phi} = \frac{\partial}{\partial \phi} \sum_{i=1}^I l_i = \sum_{i=1}^I \frac{\partial l_i}{\partial \phi} $$

#### 경사 하강법: 단계별 계산
- 경사 하강법의 단계별 계산
  - 경사 하강법의 첫 단계: 미분값 구하기
    $$ \frac{\partial L}{\partial \phi} = \frac{\partial}{\partial \phi} \sum_{i=1}^I l_i = \sum_{i=1}^I \frac{\partial l_i}{\partial \phi} $$
    - 손실 함수 $L[\phi]$의 미분을 계산: 각 데이터 손실 $l_i$에 대한 편미분의 합으로 표현
      $$ \frac{\partial l_i}{\partial \phi} = \begin{bmatrix} \frac{\partial l_i}{\partial \phi_0} \\ \frac{\partial l_i}{\partial \phi_1} \end{bmatrix} = \begin{bmatrix} 2(\phi_0 + \phi_1x_i - y_i) \\ 2x_i(\phi_0 + \phi_1x_i - y_i) \end{bmatrix} $$
  - 두번째 단계: 파라미터 업데이트
    - 미분값의 방향의 반대 방향으로 이동(손실 최소화)
      $$ \phi \longleftarrow \phi - \alpha \frac{\partial L}{\partial \phi} $$
    - $\alpha$ = 학습률 (Learning rate)
- ![경사하강법](수업자료4/3-2%20경사하강법.png)

#### 함수에 따른 최적화 난이도
- Convex vs Non-convex 최적화 문제
  - 손실 함수 모양에 따라 최적화의 난이도가 달라짐
    - Convex 문제: `전역(global) 최소값`이 유일함 → 최적화가 쉬움
    - Non-Convex 문제: `여러 개의 지역(local) 최소값` 또는 `안장점(saddle)점`이 있음 → 최적화가 어려움
  - Convex: 곡선이 항상 U자처럼 아래로 볼록. 그래프 위 임의의 두 점을 잇는 직선이 그래프 위(또는 같은 위치)로 있음.
  - Non-convex: 봉우리·골짜기·오목한 구간이 섞인 모양. 두 점을 이은 직선이 그래프 아래로 내려가는 구간이 생김.
- ![Convex vs Non-convex](수업자료4/3-4%20convex%20vs%20non-convex.png)

### 확률적 경사 하강법
#### 경사 하강법 vs 확률적 경사 하강법
- 경사 하강법의 단점
  - Non-convex문제에서 `지역(local) 최소점`에 빠지기 쉬움
    - 그림: 점(1, 2, 3)과 경로는 경사 하강법으로 Loss를 줄여가는 과정
    - 점 2에서 출발한 경사 하강 방법은 local 최소점에 빠짐
  - 매 스텝마다 `전체 데이터`에 대한 `미분값`을 구하여 `업데이트`함. 스텝별 계산양이 많음
- 대안: 전체 데이터를 한번에 쓰는 대신, 무작위로 선택한 데이터 샘플 사용
  - `확률적 경사 하강법`(Stochastic Gradient Decsent, SGD)

#### 확률적 경사 하강법의 개념
- 업데이트 방식의 차이점
  - 경사 하강법(Gradient Descent)
    - `전체 데이터셋`을 사용하여 기울기를 계산(미분)하고 파라미터 업데이트
      $$ \phi_{t+1} \longleftarrow \phi_t - \alpha \sum_{i=1}^I \frac{\partial l_i[\phi_t]}{\partial \phi} $$
  - 확률적 경사 하강법(SGD Mini-batch 버전)
    - `무작위 확률`로 샘플된 `일부 데이터(batch)`만 사용하여 기울기 계산
      $$ \phi_{t+1} \longleftarrow \phi_t - \alpha \sum_{i \in B_t} \frac{\partial l_i[\phi_t]}{\partial \phi} $$

#### 확률적 경사 하강법 결과 예시
- ![확률적 경사 하강법](수업자료4/4-2%20경사%20하강법과%20확률적%20경사%20하강법.png)

#### 확률적 경사 하강법의 특성
- 확률적 경사 하강법(SGD)의 특성
  - 국소 최솟값
    - 전체 데이터가 아닌 일부 배치로 기울기를 계산하기 때문에 노이즈가 섞여 있음
    - 노이즈가 오히려 `local minima`, `saddle point`에서 빠져나오는데 도움이 됨
  - 노이즈가 있지만 여전히 타당한 업데이트
    - 미니배치의 기울기는 정확한 전체 기울기가 아니지만, 평균적으로 올바른 방향 가리킴
    - 학습이 점진적 최적점 방향으로 수렴
  - 계산 비용 절감
    - 전체 데이터셋에서 작은 배치만 사용하므로 반복(스텝)당 연산량이 적음
    - 큰 데이터셋에서 효율적으로 학습 가능
  - 수렴 특성
    - full batch처럼 매끄럽게 수렴하지 않고, 무작위성 때문에 더 많이 진동(jitter, 지그재그)하면서 움직임
    - 전역 최솟값 근처의 좋은 해에 도달할 수 있음. Convex 문제에서는 full-batch 경사하강보다 수렴이 늦을 수 있음

### 역전파
#### 합성함수의 미분: 연쇄법칙
- 연쇄법칙
  - 파라미터는 Ω를 갖는 합성 함수를 미분하면?
    $$ y = f_1(f_0(x; \Omega)) $$
  - 핵심 아이디어: 합성함수의 변화율 = 바깥의 변화율 × 안쪽의 변화율
    $$ \frac{\partial y}{\partial \Omega} = \frac{\partial f_1(f_0(x; \Omega))}{\partial f_0(x; \Omega)} \cdot \frac{\partial f_0(x; \Omega)}{\partial \Omega} $$
  - 해석:
    - $f_1$의 입력(= $f_0$)에 대한 기울기를 구한 뒤,
    - 그 값을 $f_0$의 파라미터 Ω에 대한 기울기와 곱한다.
    - x는 파라미터가 아니므로 ∂y/∂x는 여기서 필요 없음.

#### 역전파(Backpropagation)
- 역전파란?
  - 출력 오차를 기준으로 `그래프를 거꾸로` 따라가며 `연쇄법칙`으로 각 노드(파라미터 포함)의 미분값을 계산하는 절차
- 계산 과정
  - 1. 각 단계별 계산을 분해
  - 2. 각 layer별 값($f_k$)을 계산 (Forward pass)
    - $f_0 = \beta_0 + \Omega_0x$
    - $h_1 = a[f_0]$
    - $f_1 = \beta_1 + \Omega_1h_1$
    - $h_2 = a[f_1]$
    - $f_2 = \beta_2 + \Omega_2h_2$
    - $h_3 = a[f_2]$
  - 3. 각 layer별 값($f_k$)에 대한 출력 손실($l_i$)의 미분을 구함 (Backward pass)
- ![역전파](수업자료4/5-3%20역전파.png)

### 요약 및 정리
- 경사 하강법 (Gradient Descent)
  - Step 1: 손실함수의 기울기(미분값) 계산
  - Step 2: 기울기의 반대 방향으로 파라미터 업데이트
  - 학습률($\alpha$): 한 번에 이동하는 크기 결정
- Convex 문제와 최적화
  - 2차 미분이 항상 양수 → 아래로 볼록한 형태
  - 국소 최솟값 = 전역 최솟값 → 안정적인 학습 가능
- 확률적 경사 하강법 (SGD)
  - 전체가 아닌 일부(미니배치)데이터로 기울기 계산 → 계산 비용 감소
  - 매 업데이트마다 노이즈가 섞여 국소 최솟값에서 벗어나기 유리